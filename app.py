import streamlit as st
from rag_index import build_or_load, retrieve
from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
st.set_page_config(page_title="VNA TECH SUPPORT", page_icon="üìö", layout="wide")
st.title("üìö VNA TECH SUPPORT Q&A")

uploaded = st.file_uploader("T·∫£i file PDF/PPTX", type=["pdf","pptx"], accept_multiple_files=True)
query = st.text_input("ƒê·∫∑t c√¢u h·ªèi:")

if st.button("Tr·∫£ l·ªùi"):
    if uploaded and query:
        paths = []
        for f in uploaded:
            path = f.name
            with open(path,"wb") as w: w.write(f.getbuffer())
            paths.append(path)

        index = build_or_load(paths)  # t·∫°o/l·∫•y cache index.pkl
        top = retrieve(query, index, k=10)

        # T·∫°o context t·ª´ 10 chunks
        context = "\n\n".join([c for _,_,c in top])

        # G·ªçi GPT ƒë·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n context
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role":"system","content":"B·∫°n l√† tr·ª£ l√Ω k·ªπ thu·∫≠t, tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát ch·ªâ d·ª±a tr√™n context."},
                {"role":"user","content": f"CONTEXT:\n{context}\n\nC√ÇU H·ªéI: {query}\n\nN·∫øu context thi·∫øu, n√≥i 'kh√¥ng ƒë·ªß d·ªØ li·ªáu'."}
            ],
            temperature=0.2
        )

        st.subheader("‚ú® Tr·∫£ l·ªùi")
        st.write(resp.choices[0].message.content)

        st.subheader("üîü C√°c ƒëo·∫°n tr√≠ch li√™n quan")
        for score, meta, chunk in top:
            st.markdown(f"**{meta['file']} ‚Äì chunk {meta['id']} (score={score:.2f})**")
            st.write((chunk[:800] + "...") if len(chunk) > 800 else chunk)
    else:
        st.warning("H√£y t·∫£i file v√† nh·∫≠p c√¢u h·ªèi.")
