# app.py
# VNA Tech ‚Äì Tra c·ª©u t√†i li·ªáu t·ª´ Google Drive (PDF/PPTX)
# T√°c v·ª•: ƒê·ªìng b·ªô d·ªØ li·ªáu t·ª´ Drive -> chunking -> embedding -> cache (.pkl)
#         Truy h·ªìi top-10 v√† t·∫°o c√¢u tr·∫£ l·ªùi d·ª±a tr√™n c√°c ƒëo·∫°n li√™n quan.

import os, io, json, hashlib, pickle, tempfile
from typing import Dict, Any, List, Tuple

import numpy as np
import streamlit as st
import tiktoken
from pypdf import PdfReader
from pptx import Presentation
from openai import OpenAI

# ==== C·∫•u h√¨nh chung ====
APP_TITLE = "‚úàÔ∏è VNA Tech: H·ªó tr·ª£ tra c·ª©u th√¥ng tin t√†i li·ªáu t·ª´ Google Drive"
EMBED_MODEL = "text-embedding-3-small"  # 1536 chi·ªÅu
EMBED_DIM = 1536
CHUNK_TOKENS = 400
CHUNK_OVERLAP = 80
TOP_K = 10
META_PATH = "embeddings_meta.pkl"        # Cache ch√≠nh (metadata + chunks)
# -------------------------------------------------------------------------


# ========= 1) ƒê·ªåC SECRETS AN TO√ÄN =========
def load_gsa_secrets() -> Dict[str, Any]:
    """ƒê·ªçc GOOGLE_SERVICE_ACCOUNT_JSON t·ª´ st.secrets (c√≥ th·ªÉ l√† string JSON ho·∫∑c dict)."""
    if "GOOGLE_SERVICE_ACCOUNT_JSON" not in st.secrets:
        st.error("Thi·∫øu GOOGLE_SERVICE_ACCOUNT_JSON trong Secrets.")
        st.stop()
    raw = st.secrets["GOOGLE_SERVICE_ACCOUNT_JSON"]
    if isinstance(raw, dict):
        return dict(raw)
    if isinstance(raw, str):
        try:
            return json.loads(raw)
        except Exception:
            st.error(
                "GOOGLE_SERVICE_ACCOUNT_JSON kh√¥ng ph·∫£i JSON h·ª£p l·ªá. "
                "H√£y d√πng triple quotes trong Secrets v√† gi·ªØ nguy√™n k√Ω t·ª± \\n trong private_key."
            )
            st.stop()
    st.error("GOOGLE_SERVICE_ACCOUNT_JSON ph·∫£i l√† string JSON ho·∫∑c object.")
    st.stop()


def require_secret(key: str) -> str:
    val = st.secrets.get(key) or os.getenv(key)
    if not val:
        st.error(f"Thi·∫øu {key} trong Secrets.")
        st.stop()
    return val


# ========= 2) GOOGLE DRIVE (Service Account qua PyDrive2) =========
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

def get_drive(creds_dict: Dict[str, Any]) -> GoogleDrive:
    gauth = GoogleAuth(settings={
        "client_config_backend": "service",
        "service_config": {"client_json": creds_dict},
        "save_credentials": False,
    })
    gauth.ServiceAuth()
    return GoogleDrive(gauth)

def list_drive_files(drive: GoogleDrive, folder_id: str) -> List[Dict[str, Any]]:
    q = (
        f"'{folder_id}' in parents and trashed=false and "
        "("
        "mimeType='application/pdf' or "
        "mimeType='application/vnd.openxmlformats-officedocument.presentationml.presentation'"
        ")"
    )
    files = drive.ListFile({"q": q}).GetList()
    out = []
    for f in files:
        out.append({
            "id": f["id"],
            "title": f["title"],
            "mimeType": f["mimeType"],
            "modifiedDate": f.get("modifiedDate"),
            "md5Checksum": f.get("md5Checksum"),  # c√≥ n·∫øu upload qua API
        })
    return out

def download_drive_file(drive: GoogleDrive, file_id: str, local_path: str) -> str:
    f = drive.CreateFile({"id": file_id})
    f.GetContentFile(local_path)
    return local_path


# ========= 3) X·ª¨ L√ù T√ÄI LI·ªÜU & EMBEDDING =========
def md5_of_file(path: str) -> str:
    h = hashlib.md5()
    with open(path, "rb") as fp:
        for chunk in iter(lambda: fp.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()

def read_pdf(path: str) -> List[Tuple[int, str]]:
    out = []
    reader = PdfReader(path)
    for i, p in enumerate(reader.pages, start=1):
        txt = p.extract_text() or ""
        out.append((i, txt))
    return out

def read_pptx(path: str) -> List[Tuple[int, str]]:
    prs = Presentation(path)
    out = []
    for i, slide in enumerate(prs.slides, start=1):
        parts = [sh.text for sh in slide.shapes if hasattr(sh, "text")]
        out.append((i, " ".join(parts)))
    return out

def token_chunks(text: str, max_tokens=CHUNK_TOKENS, overlap=CHUNK_OVERLAP, enc_name="cl100k_base") -> List[str]:
    enc = tiktoken.get_encoding(enc_name)
    toks = enc.encode(text)
    chunks, i, n = [], 0, len(toks)
    while i < n:
        j = min(i + max_tokens, n)
        chunks.append(enc.decode(toks[i:j]))
        if j == n: break
        i = j - overlap
    return chunks

def get_openai_client() -> OpenAI:
    key = require_secret("OPENAI_API_KEY")
    return OpenAI(api_key=key)

def embed_texts(client: OpenAI, texts: List[str]) -> np.ndarray:
    if not texts:
        return np.zeros((0, EMBED_DIM), dtype=np.float32)
    r = client.embeddings.create(model=EMBED_MODEL, input=texts)
    vecs = [d.embedding for d in r.data]
    return np.array(vecs, dtype=np.float32)

def l2_normalize(X: np.ndarray) -> np.ndarray:
    n = np.linalg.norm(X, axis=1, keepdims=True) + 1e-10
    return X / n


# ========= 4) CACHE embeddings_meta.pkl =========
# C·∫•u tr√∫c meta:
# {
#   "dim": 1536,
#   "files": {
#       <file_id>: {
#           "title": ...,
#           "md5": "...",
#           "ranges": [(start, end)],     # d·∫£i ch·ªâ s·ªë vector trong 'embeddings' & 'texts'
#       },
#       ...
#   },
#   "embeddings": np.ndarray [N, dim] (ƒë√£ chu·∫©n h√≥a),
#   "texts": List[str] (song h√†nh v·ªõi embeddings)
# }

def load_meta(path=META_PATH):
    if os.path.exists(path):
        with open(path, "rb") as f:
            return pickle.load(f)
    return {"dim": EMBED_DIM, "files": {}, "embeddings": np.zeros((0, EMBED_DIM), dtype=np.float32), "texts": []}

def save_meta(meta, path=META_PATH):
    with open(path, "wb") as f:
        pickle.dump(meta, f)

def sync_from_drive(drive: GoogleDrive, folder_id: str) -> Dict[str, Any]:
    """ƒê·ªìng b·ªô: ch·ªâ x·ª≠ l√Ω file m·ªõi/ƒë√£ thay ƒë·ªïi, sau ƒë√≥ c·∫≠p nh·∫≠t embeddings_meta.pkl"""
    client = get_openai_client()
    meta = load_meta()

    files = list_drive_files(drive, folder_id)
    added = 0

    for f in files:
        fid, title = f["id"], f["title"]
        # t·∫£i t·∫°m ƒë·ªÉ t√≠nh md5 ch·∫Øc ch·∫Øn
        with tempfile.TemporaryDirectory() as td:
            local = os.path.join(td, title)
            download_drive_file(drive, fid, local)
            md5_now = md5_of_file(local)
            need_update = fid not in meta["files"] or meta["files"][fid]["md5"] != md5_now
            if not need_update:
                continue

            # ƒê·ªçc & chunk
            pages = read_pdf(local) if title.lower().endswith(".pdf") else read_pptx(local)
            chunks = []
            for page_no, txt in pages:
                chunks.extend(token_chunks(txt))

            # Embedding & chu·∫©n h√≥a
            vecs = embed_texts(client, chunks)
            vecs = l2_normalize(vecs)

            # Append v√†o meta (embeddings + texts)
            start = meta["embeddings"].shape[0]
            if vecs.shape[0] > 0:
                meta["embeddings"] = np.vstack([meta["embeddings"], vecs])
                meta["texts"].extend(chunks)
                end = meta["embeddings"].shape[0]
            else:
                end = start

            meta["files"][fid] = {"title": title, "md5": md5_now, "ranges": [(start, end)]}
            added += (end - start)

    if added > 0:
        save_meta(meta)
    return meta


def retrieve_top_k(query: str, meta: Dict[str, Any], k: int = TOP_K) -> List[Tuple[float, str, str]]:
    """Tr·∫£ v·ªÅ list (score, file_title, chunk_text)"""
    client = get_openai_client()
    q = embed_texts(client, [query])
    q = l2_normalize(q)[0]  # (dim,)
    M = meta["embeddings"]  # (N, dim)
    if M.shape[0] == 0:
        return []
    sims = M @ q  # cosine v√¨ ƒë√£ chu·∫©n ho√°
    idx = np.argsort(-sims)[:k]
    results = []
    # √°nh x·∫° file title theo range
    file_by_index = {}
    for fid, finfo in meta["files"].items():
        title = finfo["title"]
        for (s, e) in finfo["ranges"]:
            for i in range(s, e):
                file_by_index[i] = title
    for i in idx:
        results.append((float(sims[i]), file_by_index.get(int(i), "unknown"), meta["texts"][int(i)]))
    return results


def answer_from_chunks(query: str, chunks: List[str]) -> str:
    """G·ªçi OpenAI ƒë·ªÉ tr·∫£ l·ªùi d·ª±a tr√™n c√°c chunk ƒë√£ truy h·ªìi."""
    client = get_openai_client()
    context = "\n\n".join(chunks)
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω k·ªπ thu·∫≠t. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch·ªâ d·ª±a tr√™n CONTEXT."},
            {"role": "user", "content": f"CONTEXT:\n{context}\n\nC√ÇU H·ªéI: {query}\n\nN·∫øu thi·∫øu d·ªØ li·ªáu, h√£y n√™u r√µ."}
        ],
        temperature=0.2
    )
    return resp.choices[0].message.content


# ========= 5) GIAO DI·ªÜN STREAMLIT =========
st.set_page_config(page_title="VNA Tech ‚Äì Tra c·ª©u t√†i li·ªáu", page_icon="‚úàÔ∏è", layout="wide")
st.title(APP_TITLE)

# Ki·ªÉm tra Secrets t·ªëi thi·ªÉu
DRIVE_FOLDER_ID = require_secret("DRIVE_FOLDER_ID")
creds_dict = load_gsa_secrets()

with st.sidebar:
    st.subheader("Thi·∫øt l·∫≠p")
    st.write("‚Ä¢ Ngu·ªìn: Google Drive (th∆∞ m·ª•c ƒë√£ chia s·∫ª cho Service Account).")
    if st.button("üîÑ ƒê·ªìng b·ªô Drive ‚Üí Cache (.pkl)"):
        with st.spinner("ƒêang ƒë·ªìng b·ªô d·ªØ li·ªáu t·ª´ Google Drive..."):
            drive = get_drive(creds_dict)
            meta_after = sync_from_drive(drive, DRIVE_FOLDER_ID)
            st.success(f"ƒê·ªìng b·ªô xong. S·ªë vector: {meta_after['embeddings'].shape[0]}")

st.markdown("‚Äî")
query = st.text_input("ƒê·∫∑t c√¢u h·ªèi:")
if st.button("T√¨m c√¢u tr·∫£ l·ªùi") and query:
    # B·∫£o ƒë·∫£m ƒë√£ c√≥ cache (n·∫øu ch∆∞a b·∫•m ƒê·ªìng b·ªô)
    drive = get_drive(creds_dict)
    meta = sync_from_drive(drive, DRIVE_FOLDER_ID)  # ch·ªâ x·ª≠ l√Ω file m·ªõi/ƒë·ªïi
    if meta["embeddings"].shape[0] == 0:
        st.warning("Ch∆∞a c√≥ d·ªØ li·ªáu trong cache. H√£y b·∫•m 'ƒê·ªìng b·ªô Drive ‚Üí Cache (.pkl)'.")
        st.stop()

    hits = retrieve_top_k(query, meta, k=TOP_K)
    if not hits:
        st.info("Kh√¥ng t√¨m th·∫•y ƒëo·∫°n li√™n quan.")
        st.stop()

    st.subheader("üîü C√°c ƒëo·∫°n li√™n quan")
    only_chunks = []
    for score, title, chunk in hits:
        st.markdown(f"**[{title}]** ‚Äî score={score:.3f}")
        st.write((chunk[:800] + "...") if len(chunk) > 800 else chunk)
        st.markdown("---")
        only_chunks.append(chunk)

    st.subheader("‚ú® Tr·∫£ l·ªùi")
    answer = answer_from_chunks(query, only_chunks)
    st.write(answer)
